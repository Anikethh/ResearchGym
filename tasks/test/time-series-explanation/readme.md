## Time-Series XAI

This repository provides a starting point for time-series explainability research.

### Install

Optional for synthetic data generation:
```bash
git clone https://github.com/TimeSynth/TimeSynth.git
cd TimeSynth && python setup.py install && cd -
python synthetic/switchstate/switchgenerator.py
```

### Datasets
Synthetic
- Switch-feature: 
Following the protocol of Tonekaboni et al. (2020) and the implementation details in Liu et al. (2024), synthesize the dataset with a three-state hidden Markov model (HMM) whose initial distribution is 

π = [ 1/3, 1/3, 1/3 ]

and whose transition matrix is given as follows:

[0.95 0.02 0.03
0.02 0.95 0.03
0.03 0.02 0.95]

At each time step, the model occupies a state st ∈ {0, 1, 2} and emits a three-dimensional feature vector xt drawn from a Gaussian Process (GP) mixture. Each GP component employs an RBF kernel with γ = 0.2 and marginal variance 0.1; its mean vector is state-dependent:

µ0 = [0.8, −0.5, −0.2], µ1 = [0, −1.0, 0], µ2 = [−0.2, −0.2, 0.8].

The binary label yt also depends on the state and is drawn from Bernoulli(pt) with
pt =    1/(1 + exp(−xt,0)) if st = 0,
        1/(1 + exp(−xt,1)) else if st = 1,
        1/(1 + exp(−xt,2)) else if st = 2.

Generate 1,000 time series sequences of length 100 (T = 100) using the above procedure. The dataset should be split into 800 training samples and 200 test samples for evaluating time series XAI methods. Each sample should be annotated with the true saliency map {(t, st) | t = 1, . . . , T}, as in Liu et al. (2024).

- State: 

First proposed by Tonekaboni et al. (2020) and later slightly modified by Crabbe & Van Der Schaar ´ (2021), this dataset has since become a standard test-bed for time series XAI.

Each sequence is generated by a two-state hidden Markov model (HMM) with initial distribution π = [0.5, 0.5] and transition matrix
[
0.1 0.9
0.1 0.9
]

Conditioned on the latent state st ∈ {0, 1}, an input feature vector xt is sampled from N (µst
, Σst
) with
µ0 = [0.1, 1.6, 0.5], µ1 = [−0.1, −0.4, −1.5],
Σ0 =
[0.8 0 0
0 0.8 0.01
0 0.01 0.8] , 
Σ1 =
[0.8 0.01 0
0.01 0.8 0
0 0 0.8]

A binary label yt is then drawn from Bernoulli(pt) where
pt =    1/(1 + exp(−xt,1)) if st = 0,
        1/(1 + exp(−xt,2)) else if st = 1.

Use the above procedure to synthesize 1,000 time series of length 200 (T = 200). For each sample, the true saliency map should be defined as {(t, 1 + st) | t = 1, . . . , T}, following Crabbe & Van Der Schaar ´ (2021). Train on the first 800 sequences and reserve the remaining 200 for evaluation.



Real-world
- PAM: https://dataverse.harvard.edu/file.xhtml?fileId=7495583&version=1.0
- Boiler: https://dataverse.harvard.edu/file.xhtml?fileId=7495589&version=1.0
- Epilepsy: https://dataverse.harvard.edu/file.xhtml?fileId=7495586&version=1.0
- Wafer: https://www.cs.ucr.edu/%7Eeamonn/time_series_data_2018/UCRArchive_2018.zip (From UCR archive) (Password is "someone")
- Freezer: https://www.cs.ucr.edu/%7Eeamonn/time_series_data_2018/UCRArchive_2018.zip (From UCR archive) (Password is "someone")

#### Local layout and expected files

All datamodules read from `<task-root>/data/<dataset_name>` (see `datasets/*.py`). Before training,
clone the upstream preprocessing outputs into the matching subfolders (e.g. `PAM/processed_data/*.npy`,
`boiler/split=*.pt`, `epilepsy/split_*.npy`, `Wafer/Wafer_{TRAIN,TEST}.txt`,
`FreezerRegularTrain/FreezerRegularTrain_{TRAIN,TEST}.txt`). Synthetic runners expect `data/hmm/` and
the `simulated_data_l2x/` pickles generated once via `python synthetic/switchstate/switchgenerator.py`.

### Usage
- Real datasets (e.g., PAM, Boiler, Epilepsy, Wafer, Freezer):
```bash
python real/main.py --explainers gate_mask integrated_gradients_base_abs gradientshap_abs deeplift_abs occlusion augmented_occlusion lime --data pam --train False --device cpu
```

- Synthetic datasets (HMM / Switch-Feature):
```bash
python synthetic/hmm/main.py --explainers occlusion augmented_occlusion integrated_gradients gradient_shap deep_lift lime fit dyna_mask extremal_mask gate_mask
python synthetic/switchstate/main.py --explainers occlusion augmented_occlusion integrated_gradients gradient_shap deep_lift lime fit dyna_mask extremal_mask gate_mask
```

Parsed results:
```bash
python real/parse.py --model state --data pam --top_value 100
python real/parse.py --model state --data pam --experiment_name baseline --top_value 100
```

All outputs are saved under the working directory (e.g., `results/`).

### Grading

After producing CSV logs (for example, under `logs/`), run the standardized grader to refresh
`task_description.md` and emit a JSON summary:

```bash
./grading/grade.sh                           # grade using defaults
./grading/grade.sh logs/my_run --real-explainer my_method --synthetic-explainer my_method
```

Use `--no-markdown` to inspect metrics without editing markdown, and `--json-out` to export the
summary payload. Pass `--table real`, `--table switchfeature`, or `--table state` to focus on a
subset of evaluations.

## References

Liu et al. Explaining time series via contrastive and locally sparse perturbations. (International Conference on Learning Representations, 2024)

Tonekaboni et al. What went wrong and when? instance-wise feature importance for time-series blackbox models. (Conference on Neural Information Processing Systems, 2020)

Crabbe, J. and Van Der Schaar, M. Explaining time series predictions with dynamic masks. (International
Conference on Machine Learning, 2021)
