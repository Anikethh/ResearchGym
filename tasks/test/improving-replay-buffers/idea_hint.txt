The replay buffer should be modeled as a conditional generative model over transitions, p(τ | c), where c is a relevance scalar computed by a function F(τ). Several relevance functions should be explored, including return-based (F(s, a, s′, r) = Q(s, π(s))), temporal-difference error, and intrinsic curiosity via a forward-dynamics prediction error in a latent space (F(s, a, s′, r) = 1/2 ||g(h(s), a) − h(s′)||^2). A conditional diffusion model shoulld be trained with classifier-free guidance; during training the condition is dropped with probability 0.25 to enable guided sampling at test time. The outer loop collects online experience and updates F on real data, while a periodic inner loop fits the conditional diffusion model on real transitions and generates synthetic transitions by prompting with top-k relevance values; synthetic and real transitions should be mixed with ratio r to train an off-policy learner. Replay buffers for real and synthetic data are each maintained at 1M transitions, and the generator is retrained roughly every 10K iterations. For pixel-based tasks, transitions are generated in the latent space of the policy’s CNN encoder, i.e., (fθ(s), a, fθ(s′), r). The approach is compatible with standard off-policy algorithms and scales by increasing policy capacity, synthetic data ratio, and update-to-data ratio.