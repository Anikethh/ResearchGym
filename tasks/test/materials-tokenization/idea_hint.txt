Augment the WordPiece algorithm with domain knowledge by reweighting word frequencies and re-ranking merge operations to prioritize material concepts. A material concept detector (train using a curated materials NER dataset built from 80K concepts extracted from PubChem and papers from Semantic Scholar, with noise augmentation to increase robustness) should assign probability scores to each word indicating its likelihood of being a material concept. Given a word split into subword tokens {t1, …, tn}, the label is determined as ŷ(w) = arg max over c in C of (1/n) Σ P(ti, c); if the predicted label indicates a material concept, its probability ŷmat(w) is used. Frequencies are adjusted as freq_mat(w) ← freq_origin(w) + λ • ŷmat(w) 1-ŷmat(w), increasing the priority of domain-relevant words in the merge schedule. The tokenizer then iteratively merges token pairs according to a re-ranked score (MatScore) that incorporates the adjusted frequencies until reaching the target vocabulary size. The pipeline preserves word-initial tokens and increases mean token length, yielding morphologically aligned subwords for material terms. All preceding experiments use λ = 1, based on ablation showing it is consistently optimal.