Refine query predictions by selecting the nearest gallery candidate for each query, forming query–candidate pairs to reduce overfitting and underfitting caused by large galleries. Identify source-domain-like pairs using a score that favors high intra-modality uniformity and low inter-modality gap, selecting the 30% with the smallest score. From these pairs, estimate two constraints: the modality gap of the source model and a self-adaptive entropy threshold to filter noisy predictions. A joint objective should combine three losses: an intra-modality uniformity loss that contrasts queries with their center to increase discrimination, an inter-modality gap loss that rectifies the target gap toward the estimated source gap in a non-monotonic manner, and a noise-robust adaptation loss that weights low-entropy predictions and excludes high-entropy ones using the adaptive threshold. Perform adaptation online for each incoming mini-batch, updating only normalization layers in the query encoder without accessing source data. The approach should be compatible with off-the-shelf pre-trained vision–language models.