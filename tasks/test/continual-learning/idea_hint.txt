At each task, the method decomposes low-rank updates into a magnitude (scalar coefficients) and a direction (normalized low-rank matrices) and optimizes them separately. Directions learned for previous tasks are frozen, while only their magnitudes and the current taskâ€™s new direction are trained; the layer output becomes the base weights plus a weighted sum of all preserved directions plus the new direction. This supports rehearsal-free training and direct inference with the final model, eliminating task-specific component selection. Empirically, the approach follows a low-loss trajectory toward an overlapping low-loss region across tasks, with earlier directions contributing most; theoretically, small-initialization gradient descent sequentially approximates the principal components of the optimal update. A rank-reduction variant lowers the ranks of later-task directions via predefined task indices to curb parameter growth. A distillation variant uses least-squares projection to express a new direction in the span of prior directions and, when the residual is below a threshold, absorbs it by updating magnitudes instead of expanding the direction set. All parameters are optimized end-to-end under the continual learning objective.